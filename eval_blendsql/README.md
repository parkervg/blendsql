1. the cases when blendsql outperforms gpt, what is the pattern?

The answer provided by blensql is more concise and more closely related to the table via the SQL-like syntax. Hence, as it is compared to answers of open-eneded questions, it tends to perform better for questions expecting more regulated contents from the heterogeneous table. 
For example questions that are expecting a concise numerical answer from the table.

The detailed observation level example is saved in `blendsql_win.csv`

2. when a mistake is made, do blendsql and gpt make the same mistakes?¶

Yes, judged by non-unitary denotation_acc, there are in total 
    
- 1,962 mistakes out of 3,466 total observation, which amount to a pct mistake rate of 0.57 
- 1,265 common mistakes out of 1,962 total mistakes, which amounts to a pct common mistake of 0.64

Those common mistakes are saved in `common_mistakes.csv`, an initial analysis found the error can be attributed to:
- 1. text normlaization -> reason(id): details

   - i.1) numerical(4, 3460): 6th -> sixth, 3 -> three.
   - i.2) date(3464): 17 August 1997 -> 1997-8-17.
   - i.3) unstructured lengthy answer(3, 3462): The address of the museum located in a Victorian House is 503 Peeples Street SW in the West End neighborhood of Atlanta, Georgia -> 503 Peeples Street SW
   
- 2. wrong inference
   - ii.1) using only document instead of table information(6, 3461): the billionaire information is only present in document.content, the history information is only present in document.content.
   - ii.2) blendsql syntax is too detailed on date filtering(7): harry william payne ( (1892-9-51969-7-5,p28061d) ) was.... -> %born 5 is too specific of a sql format, missing 1892-9-5 in the document.
       {{
        LLMQA(
            'What is the time difference between these two athletes?',
            (
                SELECT w."time" AS 'Time', documents.title AS 'Athlete', documents.content FROM documents
                JOIN {{
                    LLMJoin(
                        left_on='w::athlete',
                        right_on='documents::title'
                    )
                }} WHERE w."athlete" = 'josé reliegos' OR documents.content LIKE '%born 5 september 1892%' AND documents.content LIKE '%1928 olympics%'
            )
        )
- 3. wrong answer in the hybridqa table
   - iii.1) missing information (3465): Blendsql returned Craig Ferguson, which should be the correct answer because Craig Kilborn has been missing from the document.content and table w.
   - iii.2) more information(3451): Blendsql returend the full answer that is present in the table "dorohatchō 瀞八丁 kitabatake-shi yakata ato teien", but the hybridqa label the correct answer as "dorohatchō"

   
3. what type of mistakes does Blendsql make, is it in the parser or the blender?

    I think the parser worked well evidenced by zero evaluation error and valid answer output (To confirm). 
Taking a look at ~20 examples, I feel that the blendsql example generated by GPT is also informative as we expect.
The evaluation error mainly comes from the blender detailed above.   