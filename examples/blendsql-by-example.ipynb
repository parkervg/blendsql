{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:29:46.647497Z",
     "start_time": "2025-11-09T03:29:43.782398Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from blendsql.models import LlamaCpp\n",
    "from blendsql import BlendSQL\n",
    "import blendsql"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BlendSQL by Example\n",
    "\n",
    "This notebook introduces BlendSQL, and some of the usecases it can support. \n",
    "\n",
    "Importantly, the novelty of BlendSQL isn't from the ability to constrain language models according to some regular expression or context-free grammar. We can credit projects like [guidance](https://github.com/guidance-ai/guidance) and [outlines](https://github.com/dottxt-ai/outlines) for that. Instead, the novelty of BlendSQL is its ability to **infer these constraints according to the surrounding SQL syntax** and **closely align generation to the structure of the database**.\n",
    "\n",
    "SQL, as a grammar, has a lot of rules. Just take [these SQLite syntax diagrams](https://www.sqlite.org/syntaxdiagrams.html) for example. These rules include things like, `IN` statement should be followed by a list of items, `<`, `>`, should contain numerics, but `=` could contain any datatype, etc. We can use these to inform language-model functions, which we call 'ingredients', and denote in double curly brackets (`{{` and `}}`)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e929ec2204780a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Note on Models\n",
    "This demo uses LlamaCpp, and assumes access to a GPU.\n",
    "\n",
    "If you don't have the ability to run this model, take a look at [any of the other model integrations that BlendSQL supports](https://parkervg.github.io/blendsql/reference/models/models/). Importantly, note that only local models via `TransformersLLM` and `LlamaCpp` give the type constraints from the [*Play by the Type Rules* paper.](https://arxiv.org/abs/2509.20208)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "619f4d7f620a1e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T03:29:49.787524Z",
     "start_time": "2025-11-09T03:29:48.857194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LlamaCpp(\n",
    "    filename=\"google_gemma-3-4b-it-Q6_K.gguf\",\n",
    "    model_name_or_path=\"bartowski/google_gemma-3-4b-it-GGUF\",\n",
    "    config={\"n_gpu_layers\": -1, \"n_ctx\": 4096, \"seed\": 100, \"n_threads\": 16},\n",
    ")"
   ],
   "id": "9544f93b014b9cdd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (512) > n_ctx_train (0) -- possible training context overflow\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By default, loading a connection with BlendSQL will create an empty in-memory DuckDB database. As a result, we can use cool DuckDB functions like [read_text](https://duckdb.org/docs/stable/guides/file_formats/read_file#read_text).",
   "id": "e2e0e3b04ceb9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T03:06:24.821894Z",
     "start_time": "2025-11-09T03:06:23.530611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bsql = BlendSQL(\n",
    "    model=model,\n",
    ")\n",
    "smoothie = bsql.execute(\n",
    "    \"\"\"\n",
    "    SELECT {{\n",
    "        LLMQA(\n",
    "            'Describe BlendSQL in 50 words.',\n",
    "            context=(\n",
    "                SELECT content[0:5000] AS \"README\"\n",
    "                FROM read_text('https://raw.githubusercontent.com/parkervg/blendsql/main/README.md')\n",
    "            )\n",
    "        )\n",
    "    }} AS answer\n",
    "    \"\"\"\n",
    ")\n",
    "print(smoothie.df)"
   ],
   "id": "59b2d7d9c4c857e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────────────────────────────────────────┐\n",
      "│ answer                                                │\n",
      "├───────────────────────────────────────────────────────┤\n",
      "│ BlendSQL is an open-source SQL tool that combines ... │\n",
      "└───────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": "We can also set up a local database from a `Dict[str, pd.DataFrame]` object.",
   "metadata": {
    "collapsed": false
   },
   "id": "c34ef7a567ec3d81"
  },
  {
   "cell_type": "code",
   "source": [
    "bsql = BlendSQL(\n",
    "    {\n",
    "        \"People\": pd.DataFrame(\n",
    "            {\n",
    "               'Name': [\n",
    "                   'George Washington', \n",
    "                   'John Adams',\n",
    "                   'President Thomas Jefferson',\n",
    "                   'James Madison', \n",
    "                   'James Monroe', \n",
    "                   'Alexander Hamilton',\n",
    "                   'Sabrina Carpenter',\n",
    "                   'Charli XCX',\n",
    "                   'Elon Musk',\n",
    "                   'Michelle Obama',\n",
    "                   'Elvis Presley',\n",
    "               ],\n",
    "               'Known_For': [\n",
    "                   'Established federal government, First U.S. President',\n",
    "                   'XYZ Affair, Alien and Sedition Acts',\n",
    "                   'Louisiana Purchase, Declaration of Independence',\n",
    "                   'War of 1812, Constitution',\n",
    "                   'Monroe Doctrine, Missouri Compromise',\n",
    "                   'Created national bank, Federalist Papers',\n",
    "                   'Nonsense, Emails I Cant Send, Mean Girls musical',\n",
    "                   'Crash, How Im Feeling Now, Boom Clap',\n",
    "                   'Tesla, SpaceX, Twitter/X acquisition',\n",
    "                   'Lets Move campaign, Becoming memoir',\n",
    "                   '14 Grammys, King of Rock n Roll'\n",
    "               ]\n",
    "            }\n",
    "        ),\n",
    "        \"Eras\": pd.DataFrame(\n",
    "            {\n",
    "                'Years': [\n",
    "                    '1800-1900',\n",
    "                    '1900-2000',\n",
    "                    '2000-Now'\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    },\n",
    "    # This model can be changed, according to what your personal setup is\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Print the tables in our database\n",
    "for tablename in bsql.db.tables():\n",
    "    print(tablename)\n",
    "    print(blendsql.common.utils.tabulate(bsql.db.execute_to_df(f\"SELECT * FROM {tablename};\")))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:29:56.226507Z",
     "start_time": "2025-11-09T03:29:56.180885Z"
    }
   },
   "id": "b9bcd7005a97ebff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eras\n",
      "┌───────────┐\n",
      "│ Years     │\n",
      "├───────────┤\n",
      "│ 1800-1900 │\n",
      "│ 1900-2000 │\n",
      "│ 2000-Now  │\n",
      "└───────────┘\n",
      "People\n",
      "┌────────────────────────────┬──────────────────────────────────────────────────────┐\n",
      "│ Name                       │ Known_For                                            │\n",
      "├────────────────────────────┼──────────────────────────────────────────────────────┤\n",
      "│ George Washington          │ Established federal government, First U.S. President │\n",
      "│ John Adams                 │ XYZ Affair, Alien and Sedition Acts                  │\n",
      "│ President Thomas Jefferson │ Louisiana Purchase, Declaration of Independence      │\n",
      "│ James Madison              │ War of 1812, Constitution                            │\n",
      "│ James Monroe               │ Monroe Doctrine, Missouri Compromise                 │\n",
      "│ Alexander Hamilton         │ Created national bank, Federalist Papers             │\n",
      "│ Sabrina Carpenter          │ Nonsense, Emails I Cant Send, Mean Girls musical     │\n",
      "│ Charli XCX                 │ Crash, How Im Feeling Now, Boom Clap                 │\n",
      "│ Elon Musk                  │ Tesla, SpaceX, Twitter/X acquisition                 │\n",
      "│ Michelle Obama             │ Lets Move campaign, Becoming memoir                  │\n",
      "│ Elvis Presley              │ 14 Grammys, King of Rock n Roll                      │\n",
      "└────────────────────────────┴──────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Elephant in the Room - Aren't LLM Functions in SQL Super Slow?\n",
    "Short answer - compared to nearly all native SQL operations, yes. \n",
    "\n",
    "However, when using remote APIs like OpenAI or Anthropic, we can dramatically speed up processing times by batching async requests. Below demonstrates that, for a table with 17,686 rows and 1,165 unique values in the column we process, *it takes only about 6.5 seconds to run our query with gpt-4o-mini* (or about 0.005 seconds per value).\n",
    "\n",
    "By default, we allow 10 concurrent async requests. Depending on your own quotas set by the API provider, you may be able to increase this number using:\n",
    "\n",
    "```python\n",
    "from blendsql import config \n",
    "\n",
    "# Set the limit for max async calls at a given time below\n",
    "config.set_async_limit(20)\n",
    "```\n",
    "\n",
    "#### A Note on Query Optimizations\n",
    "Because LLM functions are relatively slow compared to other SQL functions, when we perform query optimizations behind the scenes, we make sure to execute all native SQL functions *before* any LLM-based functions. This ensures the language model only receives the smallest set of data it needs to faithfully evaluate a given SQL expression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19868215586a223f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17686 total rows in the table\n",
      "1165 total unique values in the 'City' column\n"
     ]
    }
   ],
   "source": [
    "db = blendsql.db.SQLite(blendsql.utils.fetch_from_hub(\"california_schools.db\"))\n",
    "print(\"{} total rows in the table\".format(db.execute_to_list(\"SELECT COUNT(*) FROM schools LIMIT 10;\")[0]))\n",
    "print(\"{} total unique values in the 'City' column\".format(db.execute_to_list(\"SELECT COUNT(DISTINCT City) FROM schools LIMIT 10;\")[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-31T12:29:56.744652Z",
     "start_time": "2025-03-31T12:29:56.630244Z"
    }
   },
   "id": "58fa4778e5d3716f"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36mExecuting \u001B[96m`SELECT * FROM schools` \u001B[36mand setting to `32d0_schools_0`...\u001B[39m\n",
      "\u001B[90mCREATE TEMP TABLE \"32d0_schools_0\" (\n",
      "\t\"CDSCode\" TEXT, \n",
      "\t\"NCESDist\" TEXT, \n",
      "\t\"NCESSchool\" TEXT, \n",
      "\t\"StatusType\" TEXT, \n",
      "\t\"County\" TEXT, \n",
      "\t\"District\" TEXT, \n",
      "\t\"School\" TEXT, \n",
      "\t\"Street\" TEXT, \n",
      "\t\"StreetAbr\" TEXT, \n",
      "\t\"City\" TEXT, \n",
      "\t\"Zip\" TEXT, \n",
      "\t\"State\" TEXT, \n",
      "\t\"MailStreet\" TEXT, \n",
      "\t\"MailStrAbr\" TEXT, \n",
      "\t\"MailCity\" TEXT, \n",
      "\t\"MailZip\" TEXT, \n",
      "\t\"MailState\" TEXT, \n",
      "\t\"Phone\" TEXT, \n",
      "\t\"Ext\" TEXT, \n",
      "\t\"Website\" TEXT, \n",
      "\t\"OpenDate\" TEXT, \n",
      "\t\"ClosedDate\" TEXT, \n",
      "\t\"Charter\" FLOAT, \n",
      "\t\"CharterNum\" TEXT, \n",
      "\t\"FundingType\" TEXT, \n",
      "\t\"DOC\" TEXT, \n",
      "\t\"DOCType\" TEXT, \n",
      "\t\"SOC\" TEXT, \n",
      "\t\"SOCType\" TEXT, \n",
      "\t\"EdOpsCode\" TEXT, \n",
      "\t\"EdOpsName\" TEXT, \n",
      "\t\"EILCode\" TEXT, \n",
      "\t\"EILName\" TEXT, \n",
      "\t\"GSoffered\" TEXT, \n",
      "\t\"GSserved\" TEXT, \n",
      "\t\"Virtual\" TEXT, \n",
      "\t\"Magnet\" FLOAT, \n",
      "\t\"Latitude\" FLOAT, \n",
      "\t\"Longitude\" FLOAT, \n",
      "\t\"AdmFName1\" TEXT, \n",
      "\t\"AdmLName1\" TEXT, \n",
      "\t\"AdmEmail1\" TEXT, \n",
      "\t\"AdmFName2\" TEXT, \n",
      "\t\"AdmLName2\" TEXT, \n",
      "\t\"AdmEmail2\" TEXT, \n",
      "\t\"AdmFName3\" TEXT, \n",
      "\t\"AdmLName3\" TEXT, \n",
      "\t\"AdmEmail3\" TEXT, \n",
      "\t\"LastUpdate\" TEXT\n",
      ")\u001B[39m\n",
      "\u001B[36mExecuting \u001B[96m `{{LLMMap('Is this in the Bay Area?', 'schools::City', options='t;f')}}`...\u001B[39m\n",
      "\u001B[90mUsing options '['t', 'f']'\u001B[39m\n",
      "Making calls to Model with batch_size 5: |\u001B[36m          \u001B[39m| 234/? [00:00<00:00, 30475.61it/s]\n",
      "\u001B[31mLLMMap with OpenaiLLM(gpt-4o-mini) only returned 1165 out of 1166 values\n",
      "\u001B[33mFinished LLMMap with values:\n",
      "{\n",
      "    \"Hayward\": true,\n",
      "    \"Newark\": true,\n",
      "    \"Oakland\": true,\n",
      "    \"Berkeley\": true,\n",
      "    \"San Leandro\": true,\n",
      "    \"-\": false,\n",
      "    \"Dublin\": true,\n",
      "    \"Fremont\": false,\n",
      "    \"Sacramento\": true,\n",
      "    \"Alameda\": null\n",
      "}\u001B[39m\n",
      "\u001B[36mCombining 1 outputs for table `schools`\u001B[39m\n",
      "\u001B[90mCREATE TEMP TABLE \"32d0_schools\" (\n",
      "\t\"CDSCode\" TEXT, \n",
      "\t\"NCESDist\" TEXT, \n",
      "\t\"NCESSchool\" TEXT, \n",
      "\t\"StatusType\" TEXT, \n",
      "\t\"County\" TEXT, \n",
      "\t\"District\" TEXT, \n",
      "\t\"School\" TEXT, \n",
      "\t\"Street\" TEXT, \n",
      "\t\"StreetAbr\" TEXT, \n",
      "\t\"City\" TEXT, \n",
      "\t\"Zip\" TEXT, \n",
      "\t\"State\" TEXT, \n",
      "\t\"MailStreet\" TEXT, \n",
      "\t\"MailStrAbr\" TEXT, \n",
      "\t\"MailCity\" TEXT, \n",
      "\t\"MailZip\" TEXT, \n",
      "\t\"MailState\" TEXT, \n",
      "\t\"Phone\" TEXT, \n",
      "\t\"Ext\" TEXT, \n",
      "\t\"Website\" TEXT, \n",
      "\t\"OpenDate\" TEXT, \n",
      "\t\"ClosedDate\" TEXT, \n",
      "\t\"Charter\" FLOAT, \n",
      "\t\"CharterNum\" TEXT, \n",
      "\t\"FundingType\" TEXT, \n",
      "\t\"DOC\" TEXT, \n",
      "\t\"DOCType\" TEXT, \n",
      "\t\"SOC\" TEXT, \n",
      "\t\"SOCType\" TEXT, \n",
      "\t\"EdOpsCode\" TEXT, \n",
      "\t\"EdOpsName\" TEXT, \n",
      "\t\"EILCode\" TEXT, \n",
      "\t\"EILName\" TEXT, \n",
      "\t\"GSoffered\" TEXT, \n",
      "\t\"GSserved\" TEXT, \n",
      "\t\"Virtual\" TEXT, \n",
      "\t\"Magnet\" FLOAT, \n",
      "\t\"Latitude\" FLOAT, \n",
      "\t\"Longitude\" FLOAT, \n",
      "\t\"AdmFName1\" TEXT, \n",
      "\t\"AdmLName1\" TEXT, \n",
      "\t\"AdmEmail1\" TEXT, \n",
      "\t\"AdmFName2\" TEXT, \n",
      "\t\"AdmLName2\" TEXT, \n",
      "\t\"AdmEmail2\" TEXT, \n",
      "\t\"AdmFName3\" TEXT, \n",
      "\t\"AdmLName3\" TEXT, \n",
      "\t\"AdmEmail3\" TEXT, \n",
      "\t\"LastUpdate\" TEXT, \n",
      "\t\"Is this in the Bay Area?\" BOOLEAN\n",
      ")\u001B[39m\n",
      "\u001B[92mFinal Query:\n",
      "SELECT \"32d0_schools\".City AS City,  \"32d0_schools\".\"Is this in the Bay Area?\"  AS \"In Bay Area?\" FROM \"32d0_schools\"\u001B[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 6.575707912445068 seconds\n",
      "┌─────────────┬────────────────┐\n",
      "│ City        │   In Bay Area? │\n",
      "├─────────────┼────────────────┤\n",
      "│ Hayward     │              1 │\n",
      "│ Newark      │              1 │\n",
      "│ Oakland     │              1 │\n",
      "│ Berkeley    │              1 │\n",
      "│ Oakland     │              1 │\n",
      "│ Oakland     │              1 │\n",
      "│ Oakland     │              1 │\n",
      "│ Hayward     │              1 │\n",
      "│ San Leandro │              1 │\n",
      "│ Hayward     │              1 │\n",
      "└─────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "smoothie = BlendSQL(\n",
    "    db, model=blendsql.models.LiteLLM('openai/gpt-4o-mini', caching=False)\n",
    ").execute(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        City,\n",
    "        {{LLMMap('Is this in the Bay Area?', City, options=('t', 'f'))}} AS 'In Bay Area?'\n",
    "    FROM schools;\n",
    "    \"\"\",\n",
    ")\n",
    "print(f\"Finished in {smoothie.meta.process_time_seconds} seconds\")\n",
    "print(blendsql.utils.tabulate(smoothie.df.head(10)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-30T16:17:06.987861923Z",
     "start_time": "2024-10-26T19:42:39.991516Z"
    }
   },
   "id": "b204aea592252c9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification with 'LLMMap' and GROUP BY', Constrained by a Column's Values\n",
    "Below, we set up a BlendSQL query leveraging the `LLMMap` ingredient. This is a unary function similar to the `LENGTH` or `ABS` functions in standard SQLite. It takes a single argument (a value from a column) and returns a transformed output, which is then assigned to a new column.\n",
    "\n",
    "Below, we set up a language-model function which takes in the values from the `Name` column of the `People` table, and outputs a value *exclusively* selected from the `Eras.Years` column."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e451e810f6f0cf14"
  },
  {
   "cell_type": "code",
   "source": [
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT GROUP_CONCAT(Name, ', ') AS 'Names',\n",
    "{{LLMMap('In which time period did the person live?', Name, options=Eras.Years)}} AS \"Lived During Classification\"\n",
    "FROM People p\n",
    "GROUP BY \"Lived During Classification\"\n",
    "\"\"\")\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:30:23.255731Z",
     "start_time": "2025-11-09T03:30:22.501943Z"
    }
   },
   "id": "4d53d9c2de48693f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────────────────────────────────────────┬───────────────────────────────┐\n",
      "│ Names                                                 │ Lived During Classification   │\n",
      "├───────────────────────────────────────────────────────┼───────────────────────────────┤\n",
      "│ George Washington, John Adams, President Thomas Je... │ 1800-1900                     │\n",
      "│ Sabrina Carpenter, Charli XCX, Elon Musk, Michelle... │ 1900-2000                     │\n",
      "└───────────────────────────────────────────────────────┴───────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constrained Decoding - The Presidents Challenge\n",
    "Why does constrained decoding matter? Imagine we want to select all the information we have in our table about the first 3 presidents of the U.S. \n",
    "In the absence of relevant data stored in our database, we turn to our language model. But one thing thwarts our plans - the language model doesn't know that we've stored the 3rd president's name in our database as `'President Thomas Jefferson'`, not `'Thomas Jefferson'`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1be8d0ae27266c3"
  },
  {
   "cell_type": "code",
   "source": [
    "# Setting `infer_gen_constraints=False` - otherwise, this counter-example would work\n",
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT * FROM People\n",
    "WHERE People.Name IN {{LLMQA('First 3 presidents of the U.S?', return_type='List[str]')}}\n",
    "\"\"\", infer_gen_constraints=False, verbose=True)\n",
    "# The final query 'SELECT * FROM People WHERE Name IN  ('George Washington','John Adams','Thomas Jefferson')' only yields 2 rows\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:30:46.394174Z",
     "start_time": "2025-11-09T03:30:46.387757Z"
    }
   },
   "id": "ea77a9ee21a00efc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36mExecuting \u001B[96m `{{LLMQA('First 3 presidents of the U.S?', return_type='List[str]')}}`...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[92mFinal Query:\n",
      "SELECT * FROM People WHERE People.Name IN ('George Washington', 'John Adams', 'Thomas Jefferson')\u001B[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────┬───────────────────────────────────────────────────────┐\n",
      "│ Name              │ Known_For                                             │\n",
      "├───────────────────┼───────────────────────────────────────────────────────┤\n",
      "│ George Washington │ Established federal government, First U.S. Preside... │\n",
      "│ John Adams        │ XYZ Affair, Alien and Sedition Acts                   │\n",
      "└───────────────────┴───────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constrained decoding comes to our rescue. By specifying `infer_gen_constraints=True` (which is the default), BlendSQL infers from the surrounding SQL syntax that we expect a value from `People.Name`, and we force the generation to only select from values present in the `Name` column - which leads to the expected response."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed4e14cb4135c095"
  },
  {
   "cell_type": "code",
   "source": [
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT * FROM People P\n",
    "WHERE P.Name IN {{LLMQA('First 3 presidents of the U.S?')}}\n",
    "\"\"\", infer_gen_constraints=True, verbose=True)\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:31:30.648022Z",
     "start_time": "2025-11-09T03:31:30.351692Z"
    }
   },
   "id": "b59038996327dc7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36mExecuting \u001B[96m `{{LLMQA('First 3 presidents of the U.S?')}}`...\u001B[39m\n",
      "\u001B[90mUsing options '{'Charli XCX', 'President Thomas Jefferson', 'John Adams', 'George Washington', 'Elon Musk', 'James Madison', 'James Monroe', 'Sabrina Carpenter', 'Elvis Presley', 'Michelle Obama', 'Alexander Hamilton'}...'\u001B[39m\n",
      "\u001B[92mFinal Query:\n",
      "SELECT * FROM People AS P WHERE P.Name IN ('George Washington', 'James Madison', 'James Monroe')\u001B[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────┬───────────────────────────────────────────────────────┐\n",
      "│ Name              │ Known_For                                             │\n",
      "├───────────────────┼───────────────────────────────────────────────────────┤\n",
      "│ George Washington │ Established federal government, First U.S. Preside... │\n",
      "│ James Madison     │ War of 1812, Constitution                             │\n",
      "│ James Monroe      │ Monroe Doctrine, Missouri Compromise                  │\n",
      "└───────────────────┴───────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constrained Decoding - The Alphabet Challenge\n",
    "\n",
    "In BlendSQL, we can utilize the power of constrained decoding to guide a language model's generation towards the structure we expect. In other words, rather than taking a \"prompt-and-pray\" approach in which we meticulously craft a natural language prompt which (hopefully) generates a list of 3 strings, we can interact with the logit space to ensure this is the case<sup>1</sup>.\n",
    "\n",
    "> [!NOTE]  \n",
    "> These guarantees are only made possible with open models, i.e. where we can access the underlying logits. For closed-models like OpenAI and Anthropic, we rely on prompting (i.e. 'Datatype: List[str]') and make predictions \"optimistically\"\n",
    "\n",
    "To demonstrate this, we can use the `LLMQA` ingredient. This ingredient optionally takes in a table subset as context, and returns either a scalar value or a list of scalars. \n",
    "\n",
    "Since BlendSQL can infer the shape of a valid generation according to the surrounding SQL syntax, when we use the `LLMQA` ingredient in a `VALUES` or `IN` clause, it will generate a list by default."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8563f8913cefc01e"
  },
  {
   "cell_type": "code",
   "source": [
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT * FROM ( VALUES {{LLMQA('What are the first few letters of the alphabet?')}} )\n",
    "\"\"\")\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:31:39.294189Z",
     "start_time": "2025-11-09T03:31:39.288901Z"
    }
   },
   "id": "b6e9868188f0c338",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────┬────────┬────────┐\n",
      "│ col0   │ col1   │ col2   │\n",
      "├────────┼────────┼────────┤\n",
      "│ A      │ B      │ C      │\n",
      "└────────┴────────┴────────┘\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so we were able to generate the first letter of the alphabet... what if we want more? \n",
    "\n",
    "Rather than modify the prompt itself (which can be quite finicky), we can leverage the regex-inspired [`quantifier` argument](Quantifier). This will take either the strings `'*'` (zero-or-more) or `'+'` (one-or-more), in addition to tighter bounds of `'{3}'` (exactly 3) or `'{1,6}'` (between 1 and 6)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea1ac65d394cf1e0"
  },
  {
   "cell_type": "code",
   "source": [
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT * FROM ( VALUES {{LLMQA('What are the first letters of the alphabet?', quantifier='{5}')}} )\n",
    "\"\"\")\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:31:41.998864Z",
     "start_time": "2025-11-09T03:31:41.992773Z"
    }
   },
   "id": "5d186b8588d96ac1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────┬────────┬────────┬────────┬────────┐\n",
      "│ col0   │ col1   │ col2   │ col3   │ col4   │\n",
      "├────────┼────────┼────────┼────────┼────────┤\n",
      "│ A      │ B      │ C      │ D      │ E      │\n",
      "└────────┴────────┴────────┴────────┴────────┘\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": "What if we want to generate the letters of a different alphabet? We can use the `options` argument for this, which takes either a reference to another column in the form `'tablename.columnname'`, or a tuple of strings.",
   "metadata": {
    "collapsed": false
   },
   "id": "55e0405c455ecb9"
  },
  {
   "cell_type": "code",
   "source": [
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT * FROM ( VALUES {{LLMQA('What are the first letters of the alphabet?', options=('α', 'β', 'γ', 'δ'), quantifier='{3}')}} )\n",
    "\"\"\")\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:31:44.886095Z",
     "start_time": "2025-11-09T03:31:44.880581Z"
    }
   },
   "id": "7ffc171b4f97bf75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────┬────────┬────────┐\n",
      "│ col0   │ col1   │ col2   │\n",
      "├────────┼────────┼────────┤\n",
      "│ α      │ β      │ γ      │\n",
      "└────────┴────────┴────────┘\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agent-Based Inference with CTE Expressions\n",
    "The above example opens up the opportunity to rewrite the query as more of an agent-based flow. SQL is a bit odd in that it's executed bottom-up, i.e. to execute the following query:\n",
    "```sql\n",
    "SELECT the_answer FROM final_table WHERE final_table.x IN \n",
    "    (SELECT some_field FROM initial_table)\n",
    "```\n",
    "...We first gather `some_field` from `initial_table`, and *then* go and fetch `the_answer`, despite the author (human or AI) having written the second step, first. This is similar to the point made by [Google in the pipe-syntax paper](https://research.google/pubs/sql-has-problems-we-can-fix-them-pipe-syntax-in-sql/) about how SQL syntactic clause order doesn't match semantic evaluation order.\n",
    "\n",
    "At the end of the day, we have two agents performing the following tasks - \n",
    "1) Brainstorm some greek letters\n",
    "2) Using the output of the previous task, select only the first 3 \n",
    "\n",
    "With BlendSQL, we can use common table expressions (CTEs) to more closely mimic this order of 'agents'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e9841576ad74398"
  },
  {
   "cell_type": "code",
   "source": [
    "smoothie = bsql.execute(\"\"\"\n",
    "WITH letter_agent_output AS (\n",
    "    SELECT * FROM (VALUES {{LLMQA('List some greek letters')}})\n",
    ") SELECT {{\n",
    "    LLMQA(\n",
    "        'What is the first letter of the alphabet?', \n",
    "        options=(SELECT * FROM letter_agent_output)\n",
    "    )\n",
    "}}\n",
    "\"\"\")\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:31:49.327587Z",
     "start_time": "2025-11-09T03:31:49.313857Z"
    }
   },
   "id": "ef5ffc11ff592e30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────┐\n",
      "│ _col_0   │\n",
      "├──────────┤\n",
      "│ α        │\n",
      "└──────────┘\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using `return_type` to Influence Generation\n",
    "BlendSQL does its best to infer datatytpes given surrounding syntax. Sometimes, though, the user may want to override those assumptions, or inject new ones that were unable to be inferred.\n",
    "\n",
    "The `return_type` argument takes a Python-style type annotation like `int`, `str`, `bool` or `float`. Below we use that to guide the generation towards one-or-more integer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449a7d3d01dea38b"
  },
  {
   "cell_type": "code",
   "source": [
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT * FROM ( VALUES {{LLMQA('Count up, starting from 1', return_type='int', quantifier='+')}} )\n",
    "\"\"\")\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:31:53.038848Z",
     "start_time": "2025-11-09T03:31:53.032830Z"
    }
   },
   "id": "658ce9487a5b4485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────┬────────┬────────┬────────┬────────┐\n",
      "│   col0 │   col1 │   col2 │   col3 │   col4 │\n",
      "├────────┼────────┼────────┼────────┼────────┤\n",
      "│      1 │      2 │      3 │      4 │      5 │\n",
      "└────────┴────────┴────────┴────────┴────────┘\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAG for Unstructured Reasoning\n",
    "In addition to using the `LLMQA` ingredient as a method for generating with tight syntax-aware constraints, we can also relax a bit and let the model give us an unstructured generation for things like summarization.\n",
    "\n",
    "Also, we can use the `context` argument to provide relevant table context. This allows us to condition generation on a curated set of data (and do cool stuff with nested reasoning)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75a215007dbff91b"
  },
  {
   "cell_type": "code",
   "source": [
    "# Give a short summary of the person who had a musical by Lin-Manuel Miranda written about them\n",
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT {{\n",
    "    LLMQA(\n",
    "        'Give me a very short summary of this person', \n",
    "        context=(\n",
    "            SELECT * FROM People \n",
    "            WHERE People.Name = {{\n",
    "                LLMQA('Who has a musical by Lin-Manuel Miranda written about them?')\n",
    "            }}\n",
    "        )\n",
    "    )\n",
    "}} AS \"Summary\"\n",
    "\"\"\", verbose=True)\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:32:06.768672Z",
     "start_time": "2025-11-09T03:32:06.536417Z"
    }
   },
   "id": "74b6331ba74b95d8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36mExecuting \u001B[96m `{{LLMQA('Give me a very short summary of this person', context=(SELECT * FROM People WHERE People.Name = {{LLMQA('Who has a musical by Lin-Manuel Miranda written about them?')}}))}}`...\u001B[39m\n",
      "\u001B[36mExecuting \u001B[96m `{{LLMQA('Who has a musical by Lin-Manuel Miranda written about them?')}}`...\u001B[39m\n",
      "\u001B[90mUsing options '{'Charli XCX', 'President Thomas Jefferson', 'John Adams', 'George Washington', 'Elon Musk', 'James Monroe', 'James Madison', 'Sabrina Carpenter', 'Elvis Presley', 'Michelle Obama', 'Alexander Hamilton'}...'\u001B[39m\n",
      "\u001B[92mFinal Query:\n",
      "SELECT * FROM People WHERE People.Name = 'Alexander Hamilton'\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[92mFinal Query:\n",
      "SELECT 'Founding father and financial leader' AS \"Summary\"\u001B[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────────────────────────┐\n",
      "│ Summary                              │\n",
      "├──────────────────────────────────────┤\n",
      "│ Founding father and financial leader │\n",
      "└──────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# A two-step reasoning problem:\n",
    "#   1) Identify who, out of the table, is a singer using `LLMMap`\n",
    "#   2) Where the previous step yields `TRUE`, select the one that wrote the song Espresso.\n",
    "smoothie = bsql.execute(\"\"\"\n",
    "WITH Musicians AS\n",
    "    (\n",
    "        SELECT Name FROM People\n",
    "        WHERE {{LLMMap('Is a singer?', Name)}} = TRUE\n",
    "    )\n",
    "SELECT Name AS \"working late cuz they're a singer\" FROM Musicians M\n",
    "WHERE M.Name = {{LLMQA('Who wrote the song \"Espresso?\"')}}\n",
    "\"\"\", verbose=True)\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:32:21.301486Z",
     "start_time": "2025-11-09T03:32:21.266270Z"
    }
   },
   "id": "1a089867c40ebc41",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36mExecuting \u001B[96m `{{LLMQA('Who wrote the song \"Espresso?\"')}}`...\u001B[39m\n",
      "\u001B[36mMaterializing CTE `Musicians`...\u001B[39m\n",
      "\u001B[36mExecuting `SELECT People.Name AS Name FROM People WHERE {{LLMMap('Is a singer?', People.Name)}} = TRUE` and setting to `Musicians`\u001B[39m\n",
      "\u001B[36mExecuting \u001B[96m `{{LLMMap('Is a singer?', People.Name)}}`...\u001B[39m\n",
      "\u001B[90mExtracted predicate literals `[True]`\u001B[39m\n",
      "\u001B[90mUsing regex '(t|f|true|false|True|False)'\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMMap with batch_size=1: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7715dfb76ff4474888a337b3b76746f7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mFinished LLMMap with values:\n",
      "{\n",
      "    \"Elon Musk\": false,\n",
      "    \"George Washington\": false,\n",
      "    \"Elvis Presley\": true,\n",
      "    \"James Monroe\": false,\n",
      "    \"Charli XCX\": true,\n",
      "    \"John Adams\": false,\n",
      "    \"James Madison\": true,\n",
      "    \"Alexander Hamilton\": true,\n",
      "    \"Sabrina Carpenter\": true,\n",
      "    \"President Thomas Jefferson\": false\n",
      "}\u001B[39m\n",
      "\u001B[36mCombining 1 outputs for table `People`\u001B[39m\n",
      "\u001B[90mCREATE OR REPLACE TEMP TABLE \"7f1e_People\" AS SELECT * FROM df\u001B[39m\n",
      "\u001B[36mCreated temp table 7f1e_People\u001B[39m\n",
      "\u001B[92mFinal Query:\n",
      "SELECT \"7f1e_People\".Name AS Name FROM \"7f1e_People\" WHERE \"7f1e_People\".\"Is a singer?\" = TRUE\u001B[39m\n",
      "\u001B[90mCREATE OR REPLACE TEMP TABLE \"Musicians\" AS SELECT * FROM df\u001B[39m\n",
      "\u001B[36mCreated temp table Musicians\u001B[39m\n",
      "\u001B[90mUsing options '{'Charli XCX', 'James Madison', 'Sabrina Carpenter', 'Elvis Presley', 'Michelle Obama', 'Alexander Hamilton'}...'\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[92mFinal Query:\n",
      "SELECT M.Name AS \"working late cuz they're a singer\" FROM Musicians AS M WHERE M.Name = 'Charli XCX'\u001B[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────────────────────┐\n",
      "│ working late cuz they're a singer   │\n",
      "├─────────────────────────────────────┤\n",
      "│ Charli XCX                          │\n",
      "└─────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Internet-Connected RAG \n",
    "So we know how to use a table subset as a context, by writing subqueries. But what if the knowledge we need to answer a question isn't present in the universe of our table?\n",
    "\n",
    "For this, we can hook up all of our ingredients with some search functions. Modifying the behavior of the default ingredients can be done by initializing it via `from_args()` call, and passing the new object in either the `bsql.execute(ingredients={new_obj})` call, or the original DB creation in `BlendSQL(ingredients={new_obj})`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14c549c0bc8e5563"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's ask a question that requires a bit more world-knowledge to answer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b18b91327f1a9874"
  },
  {
   "cell_type": "code",
   "source": [
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT * FROM People WHERE Name = {{LLMQA(\"Who's birthday is June 28, 1971?\")}}\"\"\")\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:32:31.383737Z",
     "start_time": "2025-11-09T03:32:31.143744Z"
    }
   },
   "id": "f1a798ba024c1154",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────┬─────────────────────────────────┐\n",
      "│ Name          │ Known_For                       │\n",
      "├───────────────┼─────────────────────────────────┤\n",
      "│ Elvis Presley │ 14 Grammys, King of Rock n Roll │\n",
      "└───────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not right - Elvis was born in 1935.\n",
    "\n",
    "Now let's try again, using constrained decoding via `options` and using the `RAGQA` ingredient to fetch relevant context via a [Tavily web search](https://app.tavily.com/home) first."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b006708b577b4b3"
  },
  {
   "cell_type": "code",
   "source": [
    "from blendsql.ingredients import LLMQA\n",
    "from blendsql.search import TavilySearch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Assumes we have a .env file with a `TAVILY_API_KEY` variable set.\n",
    "\n",
    "RAGQA = LLMQA.from_args(\n",
    "    searcher=TavilySearch(\n",
    "        k=2,  # Retrieve 2 document on each search\n",
    "    ),\n",
    ") # Whatever variable name we use here, we can refer to the function as that in our `execute` call\n",
    "\n",
    "smoothie = bsql.execute(\"\"\"\n",
    "SELECT * FROM People WHERE Name = {{RAGQA(\"Who's birthday is June 28, 1971?\")}}\n",
    "\"\"\", ingredients={RAGQA}, verbose=True)\n",
    "print(smoothie.df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T03:25:53.228418Z",
     "start_time": "2025-11-09T03:25:52.970483Z"
    }
   },
   "id": "882c37cb31e10085",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36mExecuting \u001B[96m `{{RAGQA(\"Who's birthday is June 28, 1971?\")}}`...\u001B[39m\n",
      "\u001B[90mRetrieved contexts '['Famous People Born on June 28, 1971 - BirthdayDBs....', 'June 28th, 1971 (Monday): Birthday, Zodiac & Weekd...']'\u001B[39m\n",
      "\u001B[90mUsing options '{'Charli XCX', 'James Madison', 'John Quincy Adams', 'Michelle Obama', 'Thomas Jefferson', 'George Washington', 'Sabrina Carpenter', 'Alexander Hamilton', 'Elon Musk', 'Elvis Presley', 'James Monroe'}...'\u001B[39m\n",
      "\u001B[35mUsing model cache...\u001B[39m\n",
      "\u001B[92mFinal Query:\n",
      "SELECT * FROM People WHERE People.Name = 'Elon Musk'\u001B[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────┬──────────────────────────────────────┐\n",
      "│ Name      │ Known_For                            │\n",
      "├───────────┼──────────────────────────────────────┤\n",
      "│ Elon Musk │ Tesla, SpaceX, Twitter/X acquisition │\n",
      "└───────────┴──────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice! Elon Musk was indeed born on June 28th, 1971. You can check out the BlendSQL logs above to validate this given the web context."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eb6f2202332c596"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
